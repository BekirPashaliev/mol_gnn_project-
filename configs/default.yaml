# -----------------------
#  Главный конфиг Stage A
# -----------------------

# Пути к данным ------------------------------------------------------
data:
  csv_path: mol_gnn_project/data/raw/compounds.csv          # сырой CSV
  dataset_pt: mol_gnn_project/data/processed/dataset.pt      # сериализованный GraphDataset
  use_cache: true                                            # загружать dataset_pt, если есть

# Сплиты --------------------------------------------------------------
split:
  train: 0.70
  val:   0.15
  test:  0.15
  random_state: 42

# Параметры модели ----------------------------------------------------
model:
  hidden_dim: 256    # размер скрытого слоя в энкодере/декодере
  num_layers: 3      # число GNN-слоёв в энкодере
  latent_dim: 64     # размер латентного вектора z

# Гиперпараметры обучения --------------------------------------------
training:
  use_gpu: true             # использовать GPU, если он доступен

  batch_size: 128          # число графов в одном батче
  adapt_bs: false          # для роста batch_size
  max_batch_size: 512     # до какого максимума можно увеличивать
  bs_increase_every: 10   # каждые N эпох удваиваем batch_size

  epochs: 50               # сколько проходов по всем данным

  lr: 1e-2                 # скорость обучения (learning rate)
  adapt_lr: true         # для вкл/выкл ReduceLROnPlateau
  lr_factor: 0.5          # во сколько раз уменьшаем LR
  lr_patience: 5          # epochs без улучшения val_loss → lr_scheduler.step()

  weight_decay: 1e-2       # L2-регуляризация для весов
  adapt_wd: true         # для адаптивного weight_decay
  wd_factor: 0.5          # во сколько раз уменьшаем WD
  wd_patience: 5          # epochs без улучшения val_loss → снижение WD

  # KL-annealing
  beta_max: 0.05            # верхний предел β (ELBO = Recon + β·KLD)
  adapt_beta: false        # для изменения β_max на лету
  max_beta: 0.5            # наверх, если хотим «дозакрутить» β
  warmup_epochs: 30        # за сколько эпох разогреваем β до beta_max
  beta_rec_thresh: 0.06    # если Recon < thresh, поднимаем beta_max
  beta_factor: 1.2         # во сколько раз увеличиваем beta_max

  # Прочее
  num_workers: 4            # DataLoader workers, число процессов для загрузки данных
  recon_mode: mse          # способ вычисления ошибки реконструкции
                           # («mse» для непрерывных признаков;
                           #  «ce» — для one-hot классов)
  patience: 15             # терпимость для early-stopping (эпохи без улучшения val-loss)

# Директории логов / чекпойнтов --------------------------------------
logging:
  log_dir: runs/            # TensorBoard
  checkpoint_dir: checkpoints/